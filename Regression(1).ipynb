{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Regression(1).ipynb","private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyNSAKg3boqzkoFHYExXU3jQ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"4Ld0nFQ5Gs4l"},"source":["##비용 최소화하기-경사 하강법(Gradient Descent) 소개"]},{"cell_type":"code","metadata":{"id":"sAofk1q_GmhG"},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","np.random.seed(0)\n","#y = 4*x+6를 근사(w1=4,w0=6). 임의의 값은 노이즈를 위해 만듦.\n","X = 2*np.random.rand(100,1)\n","y = 6 + 4*X + np.random.rand(100,1)\n","\n","#X,y 데이터 세트 산점도로 시각화\n","plt.scatter(X,y)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e8EOQhh4JmR4"},"source":["###get_weight_update 함수  \n","w1과 w0을 업데이트 할 w1_update,w0_update 를 반환"]},{"cell_type":"code","metadata":{"id":"JdO_4661Jhe1"},"source":["def get_weight_updates(w1,w0,X,y,learning_rate=0.01):\n","    N = len(y)\n","    #먼저 w1_update,w0_update를 각각 w1,w0의 shape와 동일한 크기를 가진 0값으로 초기화\n","    w1_update = np.zeros_like(w1)\n","    w0_upate = np.zeros_like(w0)\n","    #예측 배열 계산하고 예측과 실제 값의 차이 계산\n","    y_pred = np.dot(X,w1.T) + w0\n","    diff = y-y_pred\n","    \n","    #w0_update를 dot 행렬 연산으로 구하기 위해 모두 1값을 가진 행렬 생성\n","    w0_factors = np.ones((N,1))\n","    #w1과 w0을 업데이트할 w1_update와 w0_update 계산\n","    w1_update = -(2/N)*learning_rate*(np.dot(X.T,diff))\n","    w0_update = -(2/N)*learning_rate*(np.dot(w0_factors.T,diff))\n","\n","    return w1_update,w0_update"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kw6Digp8LCKY"},"source":["###gradient_descent_steps 함수  \n","경사 하강 방식으로 반복적으로 수행하여 w1과 w0를 업데이트 한다."]},{"cell_type":"code","metadata":{"id":"R4BBrBWYLBUv"},"source":["#입력 인자 iters 로 주어진 횟수만큼 반복적으로 w1과 w0를 업데이트 적용함.\n","def gradient_descent_steps(X,y,iters=1000):\n","    #w0와 w1을 모두 0으로 초기화\n","    w0 = np.zeros((1,1))\n","    w1 = np.zeros((1,1))\n","\n","    #인자로 주어진 iters 만큼 반복적으로 get_weight_updates() 호출해 w1,w0 업데이트 수행\n","    for ind in range(iters):\n","        w1_update, w0_update = get_weight_updates(w1,w0,X,y,learning_rate=0.01)\n","        w1 = w1 - w1_update\n","        w0 = w0 - w0_update\n","    return w1,w0"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KkpT4HfoL_M2"},"source":["###get_cost 함수\n","예측값과 실제값의 RSS 차이를 계산한다."]},{"cell_type":"code","metadata":{"id":"cCreXdbZIQuc"},"source":["#비용 함수를 정의해보자\n","def get_cost(y,y_pred):\n","    N = len(y)\n","    cost =np.sum(np.square(y-y_pred))/N\n","    return cost"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kEcgb8vcMLnU"},"source":["w1,w0 = gradient_descent_steps(X,y,iters=1000)\n","print(\"w1:{0:.3f} w0:{1:.3f}\".format(w1[0,0],w0[0,0]))\n","y_pred = w1[0,0]*X + w0\n","print(\"Gradient Descent Total Cost:{0:.4f}\".format(get_cost(y,y_pred)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-O9Y8507M_26"},"source":["실제 선형식인 y=4X + 6 과 유사하게 w1 = 4.028, w0 = 6.490 가 도출되었다. 예측 오류 비용은 약 0.0775 가 나온다. 앞에서 구한 y_pred에 기반해 회귀선을 그려 보겠다."]},{"cell_type":"code","metadata":{"id":"D6sEtm4yNh3e"},"source":["plt.scatter(X,y)\n","plt.plot(X,y_pred)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Aq0CYSlKTT4_"},"source":["##(미니 배치) 확률적 경사 하강법  \n","일부 데이터만 이용해 W가 업데이트되는 값을 계산한다."]},{"cell_type":"markdown","metadata":{"id":"VDatsZXVTkLj"},"source":["### stochastic_gradient_descent_steps 함수  \n","전체 X,y 데이터에서 랜덤하게 batch_size만큼 데이터를 추출해 이를 기반으로 w1_update,w0_update를 계산한다."]},{"cell_type":"code","metadata":{"id":"AifejgCaThoJ"},"source":["def stochastic_gradient_descent_steps(X,y,batch_size=10,iters=1000):\n","    w0 = np.zeros((1,1))\n","    w1 = np.zeros((1,1))\n","\n","    prev_cost = 100000\n","    iter_index = 0\n","\n","    for ind in range(iters):\n","        np.random.seed(ind)\n","        #전체 X,y 데이터에서 랜덤하게 batch_size만큼 데이터를 추출해 sample_X,sample_y로 저장\n","        stochastic_random_index = np.random.permutation(X.shape[0])\n","        sample_X = X[stochastic_random_index[0:batch_size]]\n","        sample_y = y[stochastic_random_index[0:batch_size]]\n","        #랜덤하게 batch_size만큼 추출된 데이터 기반으로 w1_update,w0_update 계산 후 업데이트\n","        w1_update,w0_update = get_weight_updates(w1,w0,sample_X,sample_y,learning_rate=0.01)\n","        w1 = w1 - w1_update\n","        w0 = w0 - w0_update\n","        \n","        return w1,w0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"u2tSEWT3PEdN"},"source":["X.shape[0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7dDWX6_AORwP"},"source":["index = np.random.permutation(X.shape[0])\n","index"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KKcqii46U-mD"},"source":["X_index = index[0:10]\n","X_index    #X 추출될 10개의 숫자의 인덱스"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nim1ca5RVHmS"},"source":["X[X_index]   #X_index를 가진 10개의 숫자가 추출됨."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6m3nGUt4VjmV"},"source":["w1,w0 = stochastic_gradient_descent_steps(X,y,iters=1000)\n","print(\"w1:\",round(w1[0,0],3),\"w0:\",round(w0[0,0],3))\n","y_pred = w1[0,0]*X +w0\n","print(\"Stochastic Gradient Descent Total Cost:{0:.4f}\".format(get_cost(y,y_pred)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UO57eSbObNFB"},"source":["## 다항 회귀와 과대적합/과소적합 이해"]},{"cell_type":"markdown","metadata":{"id":"xd8a51fqfJBw"},"source":["[x_1,x_2] = [0,1]  \n","[x_1.x_2] = [2,3]"]},{"cell_type":"code","metadata":{"id":"dotItQvFbSWc"},"source":["from sklearn.preprocessing import PolynomialFeatures\n","import numpy as np\n","\n","#다항식으로 변환한 단항식 생성, [[0,1],[2,3]]의 2x2 행렬 생성\n","X = np.arange(4).reshape(2,2)\n","print(\"일차 단항식 계수 피처:\\n\",X)\n","\n","#degree =2 인 2차 다항식으로 변환하기 위해 PolynomialFeatures를 이용해 변환\n","poly = PolynomialFeatures(degree=2)\n","poly.fit(X)\n","poly_ftr = poly.transform(X)\n","print('변환된 2차 다항식 계수 피처:\\n',poly_ftr)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LJmwZRAVd9kx"},"source":["3차 다항 회귀의 결정함수식은 다음과 같이 y = 1+2*x_1 + 3*x_1^2 + 4*x_2^3 로 설정하고 이를 위한 함수 polynomial_func() 함수를 만든다."]},{"cell_type":"code","metadata":{"id":"XgcHcc4Hen7m"},"source":["def polynomial_func(X):\n","    y = 1+2*X[:,0] + 3*X[:,0]**2 + 4*X[:,1]**3\n","    return y\n","X = np.arange(4).reshape(2,2)\n","print('일차 단항식 계수 feature: \\n',X)\n","y = polynomial_func(X)\n","print('삼차 다항식 결정값:\\n',y)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VVZHwYStdb0j"},"source":["X[:,0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IHd2jQb9i8rZ"},"source":["from sklearn.linear_model import LinearRegression"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lo1uHu5yiFbA"},"source":["#3차 다항식 변환\n","poly_ftr = PolynomialFeatures(degree=3).fit_transform(X)\n","print('3차 다항식 계수 feaeture:\\n',poly_ftr)\n","\n","#Linear Regression에 3차 다항식 계수 feature와 3차 다항식결정값으로 학습 후 회귀 계수 확인\n","model = LinearRegression()\n","model.fit(poly_ftr,y)\n","print('Polynomial 회귀 계수\\n',np.round(model.coef_,2))\n","print('Polynomial 회귀 shape:',model.coef_.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PYN9_q85jIf-"},"source":["[0.   0.18 0.18 0.36 0.54 0.72 0.72 1.08 1.62 2.34] 는 1+2*x_1 + 3*x_1^2 + 4*x_2^3 의 계수 값인 [1,2,0,3,0,0,0,0,0,4] 와는 차이가 있지만 다항 회귀로 근사하고 있음을 알 수 있다. 이처럼 사이킷런은 PolynomialFeatures로 피처를 변환한 후에 LinearRegression 클래스로 다항 회귀를 구현한다."]},{"cell_type":"markdown","metadata":{"id":"Z0EvlVc9jk4W"},"source":["바로 앞 예제와 같이 피처 변환과 선형 회귀 적용을 각각 별도로 하는 것보다는 사이킷런의 Pipeline객체를 이용해 한 번에 다항 회귀를 구현하는 것이 코드를 더 명료하게 작성하는 방법이다."]},{"cell_type":"code","metadata":{"id":"c1Ed1Mivjy-v"},"source":["from sklearn.preprocessing import PolynomialFeatures\n","from sklearn.linear_model import LinearRegression\n","from sklearn.pipeline import Pipeline\n","import numpy as np\n","\n","def polynomial_func(X):\n","    y = 1+2*X[:,0] + 3*X[:,0]**2 + 4*X[:,1]**3\n","    return y\n","\n","#Pipeline 객체로 Streamline하게 Polynomial Feature 변환과 Linear Regression을 연결\n","model = Pipeline([('poly',PolynomialFeatures(degree=3)),('linear',LinearRegression())])\n","X = np.arange(4).reshape(2,2)\n","y = polynomial_func(X)\n","\n","model = model.fit(X,y)\n","\n","print('Polynomial 회귀 계수\\n',np.round(model.named_steps['linear'].coef_,2))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"71qxufJeOaBv"},"source":[""],"execution_count":null,"outputs":[]}]}
{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Week10_파이썬머신러닝(텍스트 분석2).ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1h97hPUn1oAhyLFMu7voBVDA-SrkxLQoP","authorship_tag":"ABX9TyPOSaxu7UjJNoy4W4uPR86N"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#06 토픽 모델링(Topic Modeling)-20뉴스그룹"],"metadata":{"id":"S9w8x1GYmd1W"}},{"cell_type":"markdown","source":["머신러닝 기반의 토픽 모델은 숨겨진 주제를 효과적으로 표현할 수 있는 중심 단어를 함축적으로 추출함."],"metadata":{"id":"myKmpGfsmm8z"}},{"cell_type":"markdown","source":["**LDA 토픽 모델링**"],"metadata":{"id":"pruqF19CmwBo"}},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XpBv8s8GmUl5","executionInfo":{"status":"ok","timestamp":1652063388708,"user_tz":-540,"elapsed":8013,"user":{"displayName":"­신선민(자연과학대학 통계학과)","userId":"15538581907388309584"}},"outputId":"ae2b12aa-25a9-4907-a41d-2a8ad0e2a6c8"},"outputs":[{"output_type":"stream","name":"stdout","text":["CountVectorizer Shape: (7862, 1000)\n"]}],"source":["from sklearn.datasets import fetch_20newsgroups\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.decomposition import LatentDirichletAllocation\n","\n","#모토사이클,야구,그래픽스,윈도우즈, 중동, 기독교, 전자공학, 의학 8개 주제를 추출\n","cats =['rec.motorcycles','rec.sport.baseball','comp.graphics','comp.windows.x','talk.politics.mideast','soc.religion.christian','sci.electronics','sci.med']\n","\n","#위에서 cats 변수로 기재된 카테고리만 추출.fetch_20newsgroups()의 categories에 cats 입력\n","news_df = fetch_20newsgroups(subset='all',remove=('headers','footers','quotes'),categories=cats,random_state=0)\n","\n","#LDA는 Count기반의 벡터화만 적용합니다.\n","count_vect = CountVectorizer(max_df=0.95,max_features=1000,min_df=2,stop_words='english',ngram_range=(1,2))\n","\n","feat_vect = count_vect.fit_transform(news_df.data)\n","print('CountVectorizer Shape:',feat_vect.shape)"]},{"cell_type":"markdown","source":["토큰의 빈도가 max_df로 지정한 값을 초과 하거나 min_df로 지정한 값보다 작은 경우에는 무시한다.  \n","max_features : 추출할 단어 피처의 개수"],"metadata":{"id":"5CTOkJdzpqnt"}},{"cell_type":"markdown","source":["CountVectorizer 객체 변수인 feat_vect 모두 7862개의 문서가 1000개의 피처로 구성된 행렬 데이터"],"metadata":{"id":"9_ih5ctZrXA-"}},{"cell_type":"code","source":["#n_components : 토픽 개수\n","lda = LatentDirichletAllocation(n_components=8,random_state=0)\n","lda.fit(feat_vect)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y3HiYoFepH6w","executionInfo":{"status":"ok","timestamp":1652063967252,"user_tz":-540,"elapsed":35837,"user":{"displayName":"­신선민(자연과학대학 통계학과)","userId":"15538581907388309584"}},"outputId":"99b10630-7f41-465e-e898-3a8109e7fce5"},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["LatentDirichletAllocation(n_components=8, random_state=0)"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["print(lda.components_.shape)\n","#components_ : 개별 토픽별로 각 word 피처가 얼마나 많이 그 토픽에 할당됐는지에 대한 수치를 가진다.\n","#높은 값일수록 해당 word 피처는 그 토픽의 중심 word\n","lda.components_"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gfbSM4q1rwk9","executionInfo":{"status":"ok","timestamp":1652064051114,"user_tz":-540,"elapsed":268,"user":{"displayName":"­신선민(자연과학대학 통계학과)","userId":"15538581907388309584"}},"outputId":"1baee548-2ed5-446c-e7c0-19c26ed7a0b4"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["(8, 1000)\n"]},{"output_type":"execute_result","data":{"text/plain":["array([[3.60992018e+01, 1.35626798e+02, 2.15751867e+01, ...,\n","        3.02911688e+01, 8.66830093e+01, 6.79285199e+01],\n","       [1.25199920e-01, 1.44401815e+01, 1.25045596e-01, ...,\n","        1.81506995e+02, 1.25097844e-01, 9.39593286e+01],\n","       [3.34762663e+02, 1.25176265e-01, 1.46743299e+02, ...,\n","        1.25105772e-01, 3.63689741e+01, 1.25025218e-01],\n","       ...,\n","       [3.60204965e+01, 2.08640688e+01, 4.29606813e+00, ...,\n","        1.45056650e+01, 8.33854413e+00, 1.55690009e+01],\n","       [1.25128711e-01, 1.25247756e-01, 1.25005143e-01, ...,\n","        9.17278769e+01, 1.25177668e-01, 3.74575887e+01],\n","       [5.49258690e+01, 4.47009532e+00, 9.88524814e+00, ...,\n","        4.87048440e+01, 1.25034678e-01, 1.25074632e-01]])"]},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","source":["components_ : array[8,4000]  \n","8개의 토픽별로 1000개의 word 피처가 해당 토픽별로 연관도 값을 가지고 있다.  \n","components_array의 0번째row, 10번째 col에 있는 값   \n","= Topic #0에 대해서 피처 벡터화된 행렬에서 10번째 칼럼에 해당하는 피처가 Topic #0에 연관되는 수치 값"],"metadata":{"id":"UXX9ataJsPq1"}},{"cell_type":"markdown","source":["display_topics() : 각 토픽별로 연관도가 높은 순으로 word를 나열해보자."],"metadata":{"id":"_HOx0IXostmB"}},{"cell_type":"code","source":["def display_topics(model,feature_names, no_top_words):\n","    for topic_index,topic in enumerate(model.components_):\n","        print('Topic #',topic_index)\n","\n","        #components_array에서 가장 값이 큰 순으로 정렬했을 때, 그 값의 array 인덱스를 반환.\n","        topic_word_indexes = topic.argsort()[::-1]\n","        top_indexes = topic_word_indexes[:no_top_words]\n","\n","        #top_indexes 대상인 인덱스별로 feature_names에 해당하는 word featrue 추출 후 join으로 concat\n","        feature_concat = ' '.join([feature_names[i] for i in top_indexes])\n","        print(feature_concat)\n"],"metadata":{"id":"xl4IN06RsNvw","executionInfo":{"status":"ok","timestamp":1652064684285,"user_tz":-540,"elapsed":2,"user":{"displayName":"­신선민(자연과학대학 통계학과)","userId":"15538581907388309584"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["#CountVectorizer객체 내의 전체 word의 명칭을 get_features_names()를 통해 추출\n","feature_names = count_vect.get_feature_names()\n","len(feature_names)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zjsGcee0t9Xj","executionInfo":{"status":"ok","timestamp":1652064589095,"user_tz":-540,"elapsed":4,"user":{"displayName":"­신선민(자연과학대학 통계학과)","userId":"15538581907388309584"}},"outputId":"47eca230-ed22-44c7-aae4-8b9f777ac1bf"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n","  warnings.warn(msg, category=FutureWarning)\n"]},{"output_type":"execute_result","data":{"text/plain":["1000"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["#토픽별 가장 연관도가 높은 word를 15개만 추출\n","display_topics(lda,feature_names,15)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cJSQVv4guHgi","executionInfo":{"status":"ok","timestamp":1652064687098,"user_tz":-540,"elapsed":424,"user":{"displayName":"­신선민(자연과학대학 통계학과)","userId":"15538581907388309584"}},"outputId":"99214a6a-9e2b-4e44-f4a9-5473210d4d5c"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Topic # 0\n","year 10 game medical health team 12 20 disease cancer 1993 games years patients good\n","Topic # 1\n","don just like know people said think time ve didn right going say ll way\n","Topic # 2\n","image file jpeg program gif images output format files color entry 00 use bit 03\n","Topic # 3\n","like know don think use does just good time book read information people used post\n","Topic # 4\n","armenian israel armenians jews turkish people israeli jewish government war dos dos turkey arab armenia 000\n","Topic # 5\n","edu com available graphics ftp data pub motif mail widget software mit information version sun\n","Topic # 6\n","god people jesus church believe christ does christian say think christians bible faith sin life\n","Topic # 7\n","use dos thanks windows using window does display help like problem server need know run\n"]}]},{"cell_type":"markdown","source":["#07 문서 군집화 소개와 실습(Opinion Review 데이터 세트)"],"metadata":{"id":"QGnPHaCjvCk4"}},{"cell_type":"markdown","source":["**문서 군집화**   \n",": 비슷한 텍스트 구성의 문서를 군집화(Clustering) 하는 것  \n","학습 데이터 세트가 필요 없는 비지도학습 기반으로 동작"],"metadata":{"id":"o7xjsAssvJy8"}},{"cell_type":"markdown","source":["##Opinion Review 데이터 세트를 이용한 문서 군집화 수행하기"],"metadata":{"id":"xAY48o_evVUJ"}},{"cell_type":"markdown","source":["전체 51개의 파일이 토요타와 같은 자동차 브랜드에 대한 평가와 아이팟 나노(ipod nano)의 음질과 같은 다양한 전자 제품과 호텔 서비스 등에 대한 리뷰 내용"],"metadata":{"id":"zVrLYd_twSXi"}},{"cell_type":"code","source":["#여러 개의 파일을 DataFrame으로 로딩해보자.\n","import pandas as pd\n","import glob, os\n","\n","path = '/content/drive/MyDrive/ESAA(22-1)/Week10/Opinosis Opinion Review Data set/OpinosisDataset1.0/topics'\n","#path로 지정한 디렉터리 밑에 있는 모든 .data 파일의 파일명을 리스트로 취합.\n","all_files = glob.glob(os.path.join(path,\"*.data\"))\n","filename_list=[]\n","opinion_text = []\n","\n","#개별 파일의 파일명은 filename_list로 취합,\n","#개별 파일의 파일 내용은 DataFrame 로딩 후 sting으로 변환해 opinion_text_list로 취합\n","for file_ in all_files:\n","    #개별 파일을 읽어서 DataFrame으로 생성\n","    df = pd.read_table(file_,index_col=None,header=0,encoding='latin1')\n","\n","    #절대 경로로 주어진 확장명을 가공.\n","    #맨 마지막 .data 확장자도 제거\n","    filename_ = file_.split('/')[-1]\n","    filename = filename_.split('.')[0]\n","\n","    #파일명 list와 파일 내용 list에 파일명과 파일 내용을 추가.\n","    filename_list.append(filename)\n","    opinion_text.append(df.to_string().replace(\"    \",\"\")) #첫 공백 제거\n"],"metadata":{"id":"r_JlSMPyuiUk","executionInfo":{"status":"ok","timestamp":1652066792957,"user_tz":-540,"elapsed":735,"user":{"displayName":"­신선민(자연과학대학 통계학과)","userId":"15538581907388309584"}}},"execution_count":36,"outputs":[]},{"cell_type":"code","source":["document_df = pd.DataFrame({'filename':filename_list,'opinion_text':opinion_text})\n","document_df.iloc[4,1]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":179},"id":"PN7Ss5g_0lPp","executionInfo":{"status":"ok","timestamp":1652066794159,"user_tz":-540,"elapsed":4,"user":{"displayName":"­신선민(자연과학대학 통계학과)","userId":"15538581907388309584"}},"outputId":"8050b053-64f4-40b3-b0c2-56ffeba0b8c9"},"execution_count":37,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"headphone jack i got a clear case for it and it  i got a clear case for it and it like prvents me from being able to put the jack all the way in so the sound can b messsed up or i can get it in there and its playing well them go to move or something and it slides out .\\n0 Picture and sound quality are excellent for this typ of devic .\\n1  Great sound, easy to load & use once I read the instructions .\\n2 The music I loaded sounds clear and crisp .\\n3As for the overall sound I would give it a 8 out of 10, and the volume offers itself loud enough to tune out everything else at your local gym .\\n4sound and picture quality is great !\\n5  I notice that the audio playback has improved alot over the previous generations I could hear hissing in the third generation and didn't really remember how if sounded on the fourth generation because I didn't have it along time .\\n6  Sound quality on the built in speaker is not great but through headphones is awesome !\\n7  Video camera quality and sound are also surprising for such a small unit .\\n8 I just love how easy it is to use and the sound is nice and clear .\\n9 Great sound and awesome features and display  .\\n10   Easy to use, Video Camera, Radio, Size, Sound Quality  .\\n11  Very user friendly, great sound .\\n12  Great sound, lots of equalizer settings .\\n13  the sound quality is great and you do not need headphones at all to listen to music .\\n14Has a great sound and quality .\\n15Sound is great when attached to great speakers, except in the car .\\n16   I have a Bose system in my car and when I plug in the ipod it seems to magnify the sound from the ipod speaker instead of enhancing it through the Bose system .\\n17 When I hook it up to my Bose sounddock at home the sound is fabulous .\\n18   and the sound and quality is great !\\n19The sound quality is amazing and the picture quality will really surprise you .\\n20 It was fine quality sounds for  roof, raising  music and also insane levels of clearity for music videos or movies on the go !\\n21   The sound quality on the speaker microphone are also very good .\\n22  The sound,appearance silver are just what we wanted .\\n23  It has great sound, lots of memory and can be listened to without headphones, which is a plus when the headphones disappear !\\n24Sound is phenominal, great picture, storage, storage, storage !\\n25 Bigger picture, doesn't need a headset to hear it, but sound is fabulous through headset .\\n26 great sound and great price  .\\n27  Sound quality and replay is a marked improvement over the previous versions .\\n28Had an older nano  and this item is remarkably improved, with better functioning buttons and cleaner, more improved sound quality .\\n29 Easy to use and great sound  .\\n30   Easy to set up, lots of memory, Good Sounding  .\\n31   Sound, color, and features have made this a good Xmas purchase .\\n32 sound quality with no headphones in  This is my first iPod and I'm very happy to have made the switch from a different MP3 brand .\\n33 My only real complaint is that if you listen to the iPod without the headphones in   the sound gets a little off .\\n34They have added a lot of great features and it still has the same great sound and ease of use .\\n35 Earphones sound good, but slides out of ears .\\n36 The earphones sound good, but slides out of your ears and have to be pushed back in to get the full sound effect .\\n37  Easy to use, good value for money, great sound quality  .\\n38   I gave the duribility and sound quality low scores so that the overall rating would be low .\\n39   I am sure that if the thing worked it would have sounded great .\\n40   It is very easy to set up and use plus has great sound .\\n41 THE SOUND, THE STORAGE, THE COLOR AND O SO KOOL  .\\n42great video quality, no ear buds required internal sound system sounds great  .\\n43 Still to new to we'll see  Great product, video feature is awesome and I love how there is now internal sound and you don't have to use ear buds with it .\\n44   Love the sound quality and ease of use .\\n45Down loading was a breeze, with the sound quality being excellent .\\n46  Also got a sound dock   to go with the Nano for listening without a headset .\\n47  New User to iPOD , , , Sound is Great\\n48 Great sound and very easy to use .\\n49I've been really impressed with the sound quality .\\n50 sound quality is awesome, easy to load, easy to use  .\\n51Plus, the iPod has a lot more memory, more features, and better sound quality .\\n52   and the video quality is very good, and the sound quality is very good as well .\\n53  Sound quality, video quality, easy to use, space, customizable  .\\n54  The colors are awesome, the sound is great and it hold tons of songs and videos !\\n55My daughter gave this to me and my husband for Christmas, what a great gift, very easy to use, sound is great .\\n56 the video recorder, sound, look  .\\n57 the battery life  this was an awsome christmas present and i love it its really easy to use and sounds great .\\n58 Very cool features, sound quality, small size, virtually weightless  .\\n59Love the Sound, Photos Look Great .\\n60   easy to use, great sound, compact  .\\n61 It is compact and the sound quality is very good .\\n62  Sound quality is excellent, and if you take the headphones off, music will play through a built, in speaker .\\n63 The iPod Nano is a very sleek, cutting edge device with high, quality sound and state, of, the, art graphics .\\n64 EASY, FUN, SOUND QUALITY, SMALL  .\\n65  I can't compare to any others, but the sound quality is really good .\\n66  Doesn't sound too good, but it's nice to have if you need to listen to something right away !\\n67  Only things I wish they added is measuring distance [not just steps] on the pedometer, better sound on the speaker, and a  capturing still images  with the video recording feature .\\n68   The gym music drowns out the sound .\\n69I love the sound quality of the earbuds, but they fit horribly in my ears .\\n70Very easy to use, love the pedometer, lightweight, thin, nice screen, great sound, battery lasts much longer than stated  .\\n71  Easy to use, Great sound, long battery life, love the fact that I can tune in to my favorite radio station, and I'm into exercise so the pedometer with calories burned is a great addition .\\n72 The Headphone Jack is bad out of the box, sound crackles with the lightest touch of the headphone jack which makes the radio useless because they use the headphone cable as the antenna .\\n73 Turn the volume up and the sound in the headphone lowers, turn the volume down and the sound goes up .\\n74Video is impressive, speaker sound is good for size, easy menus, screen is big enough to watch movies .\\n75   the sound quality is very good .\\n76 i was surprised, b c thats one of the downsides i found through time from other customers, but im happy with the quality of sound, i also think adding noise cancel, ear buds phones would greatly improve the sound .\\n77 The sound quality is good, and\\n78  The sound is magnificent and I can do so many\\n79I particularly love the sound quality .\\n80  its really good up to apples usual standard sound quality and video quality is brillant and is ideal for anyone !\\n81  Good sound, looks great, fairly easy to use .\\n82Long battery life, Decent camera for such a small device, excellent sound quality and easy to use !\\n83  I get fabulous sound and all data about the music displays on my stereo display .\\n84  It is of the best buys I've done, sound quality, video and radio is excellent\\n85   Great sound quality, love the video recording too  :\\n86 He was able to figure everything out on his own with very little help and the sound system\\n87Received my new iPod nano and was pleased with the radio, camera, pedometer, sound quality and appearance .\\n88 No sound comes through the headphone jack when headphones are plugged in, but sound can be\\n89Poor Sound Quality This product is used every day .\\n90  Easy To Set Up, Simple Controls, Sleek Compact, Great Sound I am truly enjoying my iPod .\\n91  Great Sound, Sleek Compact The only problem so far is that our Ipod speakers don't charge the new Ipod nano .\\n92  Long Battery Life, Sleek Compact, Easy To Set Up, Reliable Performance, Great Sound, Lots of Storage bought it for the gym and it was just the right weight !\\n93  The 5th gen nano has improved on the 4th gen in a few ways, mainly in sound quality .\\n94 The nano is definitely several notches above the sound\\n95  I don't particularly use it with headphones although the sound is\\n96   No way to balance sound between ear pieces .\\n97Sound in one side is significantly higher than the other side .\\n98   I have used the video camera nd the sound is good considering and the picture is\\n99  Good sound and useful as a radio but that\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":37}]},{"cell_type":"code","source":["document_df.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"sLign2g41WU6","executionInfo":{"status":"ok","timestamp":1652066799635,"user_tz":-540,"elapsed":374,"user":{"displayName":"­신선민(자연과학대학 통계학과)","userId":"15538581907388309584"}},"outputId":"89ea96bd-26ed-4c0e-afb9-a1ae638d3da2"},"execution_count":38,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                          filename  \\\n","0       battery-life_ipod_nano_8gb   \n","1  directions_garmin_nuvi_255W_gps   \n","2      battery-life_netbook_1005ha   \n","3     display_garmin_nuvi_255W_gps   \n","4              sound_ipod_nano_8gb   \n","\n","                                        opinion_text  \n","0     short battery life  I moved up from an 8gb ...  \n","1    You also get upscale features like spoken di...  \n","2  6GHz 533FSB cpu, glossy display, 3, Cell 23Wh ...  \n","3   3 quot  widescreen display was a bonus .\\n0  ...  \n","4  headphone jack i got a clear case for it and i...  "],"text/html":["\n","  <div id=\"df-27cd7816-b6de-4bcb-93e3-1a897a0ae36f\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>filename</th>\n","      <th>opinion_text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>battery-life_ipod_nano_8gb</td>\n","      <td>short battery life  I moved up from an 8gb ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>directions_garmin_nuvi_255W_gps</td>\n","      <td>You also get upscale features like spoken di...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>battery-life_netbook_1005ha</td>\n","      <td>6GHz 533FSB cpu, glossy display, 3, Cell 23Wh ...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>display_garmin_nuvi_255W_gps</td>\n","      <td>3 quot  widescreen display was a bonus .\\n0  ...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>sound_ipod_nano_8gb</td>\n","      <td>headphone jack i got a clear case for it and i...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-27cd7816-b6de-4bcb-93e3-1a897a0ae36f')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-27cd7816-b6de-4bcb-93e3-1a897a0ae36f button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-27cd7816-b6de-4bcb-93e3-1a897a0ae36f');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":38}]},{"cell_type":"code","source":["ex_file_path = '/content/drive/MyDrive/ESAA(22-1)/Week10/Opinosis Opinion Review Data set/OpinosisDataset1.0/topics/accuracy_garmin_nuvi_255W_gps.txt.data'\n","file_path_1 = ex_file_path.split('/')[-1]\n","file_path_1"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"qKFC3xoEzxF4","executionInfo":{"status":"ok","timestamp":1652066337680,"user_tz":-540,"elapsed":281,"user":{"displayName":"­신선민(자연과학대학 통계학과)","userId":"15538581907388309584"}},"outputId":"d9dd3c0e-105f-4399-aa3d-821331c45f5c"},"execution_count":23,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'accuracy_garmin_nuvi_255W_gps.txt.data'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":23}]},{"cell_type":"code","source":["file_path_2 = file_path_1.split('.')[0]\n","file_path_2"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"oCSUvHJe0Y4K","executionInfo":{"status":"ok","timestamp":1652066364182,"user_tz":-540,"elapsed":394,"user":{"displayName":"­신선민(자연과학대학 통계학과)","userId":"15538581907388309584"}},"outputId":"1e995a99-bb65-4d89-f4ef-e98cbe861566"},"execution_count":25,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'accuracy_garmin_nuvi_255W_gps'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":25}]},{"cell_type":"markdown","source":["문서를 TF-IDF 형태로 피처 벡터화해보자.  \n","tokenizer는 이전 예제에서 Lemmatization을 구현한 LemNormalize() 함수를 이용할 것"],"metadata":{"id":"h_FgVuoO1uoq"}},{"cell_type":"code","source":["from nltk.stem import WordNetLemmatizer\n","import nltk\n","import string\n","\n","#단어 원형 추출 함수\n","lemmar = WordNetLemmatizer()\n","def LemTokens(tokens):\n","    return [lemmar.lemmatize(token) for token in tokens]\n","\n","#특수 문자 사전 생성: {33: None ...}\n","#ord() : 아스키 코드 생성\n","remove_punct_dict = dict((ord(punct),None) for punct in string.punctuation)\n","\n","#LemNormalize() 함수\n","def LemNormalize(text):\n","    #텍스트 소문자 변경 후 특수 문자 제거\n","    text_new = text.lower().translate(remove_punct_dict)\n","\n","    #단어 토큰화\n","    word_tokens = nltk.word_tokenize(text_new)\n","\n","    #단어 원형 추출\n","    return LemTokens(word_tokens)"],"metadata":{"id":"MxkTPhr_0bJQ","executionInfo":{"status":"ok","timestamp":1652067228499,"user_tz":-540,"elapsed":276,"user":{"displayName":"­신선민(자연과학대학 통계학과)","userId":"15538581907388309584"}}},"execution_count":39,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","*   string.punctuation : 느낌표, 물음표, 더하기 등의 문자들\n","*   dict((ord(punct),None) for punct in string.punctuation) : 해당 문자 사전을 생성\n","*   text.lower().translate(remove_punct_dict) 로 문자 사전에 따라 None 으로 변환한다.\n","*   단어 토큰화 후에 토큰 별로 원형을 추출한다.\n","\n","\n","\n"],"metadata":{"id":"J00R1kaT4XaT"}},{"cell_type":"code","source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","tfidf_vect = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english',\n","                             ngram_range=(1,2),min_df=0.05,max_df=0.85)\n","\n","#opinion_text 칼럼 값으로 피처 벡터화 수행\n","feature_vect = tfidf_vect.fit_transform(document_df['opinion_text'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"b3lkVplF4Vae","executionInfo":{"status":"error","timestamp":1652067468389,"user_tz":-540,"elapsed":279,"user":{"displayName":"­신선민(자연과학대학 통계학과)","userId":"15538581907388309584"}},"outputId":"7569d7d2-8ff3-41ce-82b7-d413df653e30"},"execution_count":40,"outputs":[{"output_type":"error","ename":"LookupError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-40-eddddebf3bc9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#opinion_text 칼럼 값으로 피처 벡터화 수행\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mfeature_vect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfidf_vect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'opinion_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   2075\u001b[0m         \"\"\"\n\u001b[1;32m   2076\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2077\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2078\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2079\u001b[0m         \u001b[0;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1328\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1330\u001b[0;31m         \u001b[0mvocabulary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_count_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfixed_vocabulary_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1332\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1199\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1200\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1201\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1202\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1203\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mngrams\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstop_words\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-39-d2ac8a028c36>\u001b[0m in \u001b[0;36mLemNormalize\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m#단어 토큰화\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mword_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_new\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m#단어 원형 추출\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserver_line\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \"\"\"\n\u001b[0;32m--> 128\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m     return [token for sent in sentences\n\u001b[1;32m    130\u001b[0m             for token in _treebank_word_tokenizer.tokenize(sent)]\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \"\"\"\n\u001b[0;32m---> 94\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tokenizers/punkt/{0}.pickle'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    832\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    833\u001b[0m     \u001b[0;31m# Load the resource.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 834\u001b[0;31m     \u001b[0mopened_resource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    835\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    836\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'raw'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    950\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'nltk'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    953\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'file'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    671\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'*'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/lib/nltk_data'\n    - ''\n**********************************************************************\n"]}]},{"cell_type":"code","source":["from nltk.stem import WordNetLemmatizer\n","import nltk\n","import string\n","\n","remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n","lemmar = WordNetLemmatizer()\n","\n","def LemTokens(tokens):\n","    return [lemmar.lemmatize(token) for token in tokens]\n","\n","def LemNormalize(text):\n","    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))\n","\n","#################\n","## 벡터화\n","#################\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","tfidf_vect = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english' , \\\n","                             ngram_range=(1,2), min_df=0.05, max_df=0.85 )\n","\n","#opinion_text 컬럼값으로 feature vectorization 수행\n","feature_vect = tfidf_vect.fit_transform(document_df['opinion_text'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":662},"id":"XAJ5Bb335QAh","executionInfo":{"status":"error","timestamp":1652067898270,"user_tz":-540,"elapsed":286,"user":{"displayName":"­신선민(자연과학대학 통계학과)","userId":"15538581907388309584"}},"outputId":"3e78006f-cafe-43d3-fa0e-02ea93a3dc4e"},"execution_count":41,"outputs":[{"output_type":"error","ename":"LookupError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-41-f2edfd932073>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m#opinion_text 컬럼값으로 feature vectorization 수행\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mfeature_vect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfidf_vect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'opinion_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   2075\u001b[0m         \"\"\"\n\u001b[1;32m   2076\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2077\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2078\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2079\u001b[0m         \u001b[0;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1328\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1330\u001b[0;31m         \u001b[0mvocabulary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_count_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfixed_vocabulary_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1332\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1199\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1200\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1201\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1202\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1203\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mngrams\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstop_words\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-41-f2edfd932073>\u001b[0m in \u001b[0;36mLemNormalize\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mLemNormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mLemTokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremove_punct_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m#################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserver_line\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \"\"\"\n\u001b[0;32m--> 128\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m     return [token for sent in sentences\n\u001b[1;32m    130\u001b[0m             for token in _treebank_word_tokenizer.tokenize(sent)]\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \"\"\"\n\u001b[0;32m---> 94\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tokenizers/punkt/{0}.pickle'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    832\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    833\u001b[0m     \u001b[0;31m# Load the resource.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 834\u001b[0;31m     \u001b[0mopened_resource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    835\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    836\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'raw'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    950\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'nltk'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    953\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'file'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    671\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'*'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/lib/nltk_data'\n    - ''\n**********************************************************************\n"]}]},{"cell_type":"markdown","source":["#08 문서 유사도"],"metadata":{"id":"7F0pUKMdCvXc"}},{"cell_type":"markdown","source":["##문서 유사도 측정 방법 - 코사인 유사도"],"metadata":{"id":"PNDtH9g1Cxx2"}},{"cell_type":"markdown","source":["문서와 문서 간의 유사도 비교는 일반적으로 코사인 유사도(Cosine Similarity)를 사용한다.   \n","코사인 유사도는 벡터와 벡터 간의 유사도를 비교할 때 벡터의 크기보다는 벡터의 상호 방향성이 얼마나 유사한지에 기반한다.즉, 코사인 유사도는 두 벡터 사이의 사잇각을 구해서 얼마나 유사한지 수치로 적용한 것"],"metadata":{"id":"gsCUUuN7C1c0"}},{"cell_type":"markdown","source":["##두 벡터 사잇각"],"metadata":{"id":"17KCDI8xDIGM"}},{"cell_type":"code","source":["#cos_similarity() 함수 \n","#두 개의 넘파이 배열에 대한 코사인 유사도를 구한다.\n","import numpy as np\n","\n","def cos_similarity(v1,v2):\n","    dot_product = np.dot(v1,v2)\n","    l2_norm = (np.sqrt(sum(np.square(v1)))*np.sqrt(sum(np.square(v2))))\n","    similarity = dot_product/l2_norm\n","    \n","    return similarity"],"metadata":{"id":"OQUYUSuOCwqN","executionInfo":{"status":"ok","timestamp":1652070622896,"user_tz":-540,"elapsed":267,"user":{"displayName":"­신선민(자연과학대학 통계학과)","userId":"15538581907388309584"}}},"execution_count":50,"outputs":[]},{"cell_type":"markdown","source":["doc_list로 정의된 3개의 간단한 문서의 유사도를 비교하기 위해 이 문서를 TF-IDF로 벡터화된 행렬로 변환한다."],"metadata":{"id":"Fuk6kRvLDkCq"}},{"cell_type":"code","source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","doc_list = ['if you take the blue pill, the story ends',\n","            'if you take the red pill, you stay in Wonderland',\n","            'if you take the red pill, I show you how deep the rabbit hole goes']\n","\n","tfidf_vect_simple = TfidfVectorizer()\n","feature_vect_simple = tfidf_vect_simple.fit_transform(doc_list)\n","print(feature_vect_simple.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x0mfv_OODitE","executionInfo":{"status":"ok","timestamp":1652070321060,"user_tz":-540,"elapsed":271,"user":{"displayName":"­신선민(자연과학대학 통계학과)","userId":"15538581907388309584"}},"outputId":"cf389085-4532-4a08-fd73-7b955addc327"},"execution_count":44,"outputs":[{"output_type":"stream","name":"stdout","text":["(3, 18)\n"]}]},{"cell_type":"markdown","source":["반환된 행렬은 희소 행렬이므로 앞에서 작성한 cos_similarity() 함수의 인자인 array로 만들기 위해 밀집 행렬로 변환한 뒤 다시 각각을 배열로 변환한다."],"metadata":{"id":"cA8sg-m5EKE5"}},{"cell_type":"code","source":["#TfidfVectorizer로 transform() 한 결과는 희소 행렬이므로 밀집 행렬로 변환\n","feature_vect_dense = feature_vect_simple.todense()\n","feature_vect_dense"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sJGsETVCEGCR","executionInfo":{"status":"ok","timestamp":1652070457111,"user_tz":-540,"elapsed":282,"user":{"displayName":"­신선민(자연과학대학 통계학과)","userId":"15538581907388309584"}},"outputId":"089b2871-9a5b-4722-f903-84dab565297c"},"execution_count":45,"outputs":[{"output_type":"execute_result","data":{"text/plain":["matrix([[0.4155636 , 0.        , 0.4155636 , 0.        , 0.        ,\n","         0.        , 0.24543856, 0.        , 0.24543856, 0.        ,\n","         0.        , 0.        , 0.        , 0.4155636 , 0.24543856,\n","         0.49087711, 0.        , 0.24543856],\n","        [0.        , 0.        , 0.        , 0.        , 0.        ,\n","         0.        , 0.23402865, 0.39624495, 0.23402865, 0.        ,\n","         0.3013545 , 0.        , 0.39624495, 0.        , 0.23402865,\n","         0.23402865, 0.39624495, 0.4680573 ],\n","        [0.        , 0.30985601, 0.        , 0.30985601, 0.30985601,\n","         0.30985601, 0.18300595, 0.        , 0.18300595, 0.30985601,\n","         0.23565348, 0.30985601, 0.        , 0.        , 0.18300595,\n","         0.3660119 , 0.        , 0.3660119 ]])"]},"metadata":{},"execution_count":45}]},{"cell_type":"code","source":["#첫 번째 문장과 두 번째 문장의 피처 벡터 추출\n","vect1 = np.array(feature_vect_dense[0]).reshape(-1, )\n","vect2 = np.array(feature_vect_dense[1]).reshape(-1, )\n","vect1"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"633ZavV5EolG","executionInfo":{"status":"ok","timestamp":1652070540084,"user_tz":-540,"elapsed":277,"user":{"displayName":"­신선민(자연과학대학 통계학과)","userId":"15538581907388309584"}},"outputId":"af7b6e28-bdc4-4e6d-bfdb-ce916bae0473"},"execution_count":48,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0.4155636 , 0.        , 0.4155636 , 0.        , 0.        ,\n","       0.        , 0.24543856, 0.        , 0.24543856, 0.        ,\n","       0.        , 0.        , 0.        , 0.4155636 , 0.24543856,\n","       0.49087711, 0.        , 0.24543856])"]},"metadata":{},"execution_count":48}]},{"cell_type":"code","source":["#첫 번째 문장과 두 번째 문장의 피처 벡터로 두 개 문장의 코사인 유사도 추출\n","similarity_simple = cos_similarity(vect1,vect2)\n","print('문장1,문장2 Cosine 유사도:{0:.3f}'.format(similarity_simple))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"60uywu6sExfX","executionInfo":{"status":"ok","timestamp":1652070626661,"user_tz":-540,"elapsed":319,"user":{"displayName":"­신선민(자연과학대학 통계학과)","userId":"15538581907388309584"}},"outputId":"f50475ec-cfd2-4c7c-d322-fb1473822db3"},"execution_count":51,"outputs":[{"output_type":"stream","name":"stdout","text":["문장1,문장2 Cosine 유사도:0.402\n"]}]},{"cell_type":"code","source":["#첫 번째 문장과 세 번째 문장의 피처 벡터로 두 개 문장의 코사인 유사도 추출\n","vect1 = np.array(feature_vect_dense[0]).reshape(-1, )\n","vect3 = np.array(feature_vect_dense[2]).reshape(-1, )\n","similarity_simple = cos_similarity(vect1,vect3)\n","print('문장1,문장3 Cosine 유사도:{0:.3f}'.format(similarity_simple))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YDtGLBOHFPcE","executionInfo":{"status":"ok","timestamp":1652070687238,"user_tz":-540,"elapsed":272,"user":{"displayName":"­신선민(자연과학대학 통계학과)","userId":"15538581907388309584"}},"outputId":"607a90ab-26f2-48c7-8976-02a8ef9ded29"},"execution_count":52,"outputs":[{"output_type":"stream","name":"stdout","text":["문장1,문장3 Cosine 유사도:0.404\n"]}]},{"cell_type":"code","source":["#두 번째 문장과 세 번째 문장의 피처 벡터로 두 개 문장의 코사인 유사도 추출\n","vect2 = np.array(feature_vect_dense[1]).reshape(-1, )\n","vect3 = np.array(feature_vect_dense[2]).reshape(-1, )\n","similarity_simple = cos_similarity(vect2,vect3)\n","print('문장2,문장3 Cosine 유사도:{0:.3f}'.format(similarity_simple))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4PeA3f_rFh5M","executionInfo":{"status":"ok","timestamp":1652070712159,"user_tz":-540,"elapsed":279,"user":{"displayName":"­신선민(자연과학대학 통계학과)","userId":"15538581907388309584"}},"outputId":"8673bd71-164c-41dc-d9cc-88d24133916d"},"execution_count":54,"outputs":[{"output_type":"stream","name":"stdout","text":["문장2,문장3 Cosine 유사도:0.456\n"]}]},{"cell_type":"markdown","source":["사이킷런은 코사인 유사도를 측정하기 위해 sklearn.metrics.pairwise.cosine_similarity API 제공한다.  \n","첫 번째 파라미터 : 비교 기준이 되는 문서의 피처 행렬  \n","두 번째 파라미터 : 비교되는 문서의 피처 행렬  \n","cosine_similarity()는 희소 행렬, 밀집 행렬 모두가 가능하며, 행렬 또는 배열 모두 가능하다."],"metadata":{"id":"esHLZ9s4FpKc"}},{"cell_type":"code","source":["from sklearn.metrics.pairwise import cosine_similarity\n","\n","similarity_simple_pair = cosine_similarity(feature_vect_simple[0],feature_vect_simple)\n","print(similarity_simple_pair)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TXl2fqBQFmr8","executionInfo":{"status":"ok","timestamp":1652070858535,"user_tz":-540,"elapsed":266,"user":{"displayName":"­신선민(자연과학대학 통계학과)","userId":"15538581907388309584"}},"outputId":"d54cb11e-d48e-4992-bdcd-b804040186b0"},"execution_count":55,"outputs":[{"output_type":"stream","name":"stdout","text":["[[1.         0.40207758 0.40425045]]\n"]}]},{"cell_type":"code","source":["from sklearn.metrics.pairwise import cosine_similarity\n","\n","similarity_simple_pair = cosine_similarity(feature_vect_simple[0],feature_vect_simple)\n","print(similarity_simple_pair)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UDsElFaeGLt5","executionInfo":{"status":"ok","timestamp":1652070884922,"user_tz":-540,"elapsed":280,"user":{"displayName":"­신선민(자연과학대학 통계학과)","userId":"15538581907388309584"}},"outputId":"cf7d72b7-34ac-42b2-e6df-d6273ae3254a"},"execution_count":56,"outputs":[{"output_type":"stream","name":"stdout","text":["[[0.40207758 0.40425045]]\n"]}]},{"cell_type":"code","source":["similarity_simple_pair = cosine_similarity(feature_vect_simple,feature_vect_simple)\n","print(similarity_simple_pair)\n","print('shape:',similarity_simple_pair.shape)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"At1uX6wPGSJZ","executionInfo":{"status":"ok","timestamp":1652070992357,"user_tz":-540,"elapsed":276,"user":{"displayName":"­신선민(자연과학대학 통계학과)","userId":"15538581907388309584"}},"outputId":"df156dbc-0be8-46c0-ece8-e8f41f935fa6"},"execution_count":57,"outputs":[{"output_type":"stream","name":"stdout","text":["[[1.         0.40207758 0.40425045]\n"," [0.40207758 1.         0.45647296]\n"," [0.40425045 0.45647296 1.        ]]\n","shape: (3, 3)\n"]}]},{"cell_type":"markdown","source":["#09 한글 텍스트 처리 - 네이버 영화 평점 감성 분석"],"metadata":{"id":"qWlkqeQClaEu"}},{"cell_type":"code","source":["import pandas as pd\n","\n","train_df = pd.read_csv('/content/drive/MyDrive/ESAA(22-1)/Week10/네이버 영화 평점/ratings_train.txt',sep='\\t')\n","train_df.head(3)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":143},"id":"J1Wo8Vn-GsWU","executionInfo":{"status":"ok","timestamp":1652079134009,"user_tz":-540,"elapsed":1925,"user":{"displayName":"­신선민(자연과학대학 통계학과)","userId":"15538581907388309584"}},"outputId":"602313d7-bf66-496c-f579-fdfc5f2519d7"},"execution_count":58,"outputs":[{"output_type":"execute_result","data":{"text/plain":["         id                           document  label\n","0   9976970                아 더빙.. 진짜 짜증나네요 목소리      0\n","1   3819312  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나      1\n","2  10265843                  너무재밓었다그래서보는것을추천한다      0"],"text/html":["\n","  <div id=\"df-9fe05a6d-5f58-4ca8-b405-4a6d180771b9\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>document</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>9976970</td>\n","      <td>아 더빙.. 진짜 짜증나네요 목소리</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>3819312</td>\n","      <td>흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>10265843</td>\n","      <td>너무재밓었다그래서보는것을추천한다</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9fe05a6d-5f58-4ca8-b405-4a6d180771b9')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-9fe05a6d-5f58-4ca8-b405-4a6d180771b9 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-9fe05a6d-5f58-4ca8-b405-4a6d180771b9');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":58}]},{"cell_type":"code","source":["train_df['label'].value_counts()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TFpthOcklvw_","executionInfo":{"status":"ok","timestamp":1652079149955,"user_tz":-540,"elapsed":280,"user":{"displayName":"­신선민(자연과학대학 통계학과)","userId":"15538581907388309584"}},"outputId":"d78bb05e-f321-49eb-c13e-95c39acc254c"},"execution_count":59,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0    75173\n","1    74827\n","Name: label, dtype: int64"]},"metadata":{},"execution_count":59}]},{"cell_type":"code","source":["train_df.isnull().sum()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eI2cmEwbl-eB","executionInfo":{"status":"ok","timestamp":1652079210763,"user_tz":-540,"elapsed":844,"user":{"displayName":"­신선민(자연과학대학 통계학과)","userId":"15538581907388309584"}},"outputId":"47d9c932-3560-42af-b4e7-2a67255fcea1"},"execution_count":61,"outputs":[{"output_type":"execute_result","data":{"text/plain":["id          0\n","document    5\n","label       0\n","dtype: int64"]},"metadata":{},"execution_count":61}]},{"cell_type":"markdown","source":["0과 1의 비율이 어느 한쪽으로 치우치지 않고 균등한 분포를 나타내고 있다."],"metadata":{"id":"f0awC-Z9l1qQ"}},{"cell_type":"markdown","source":["\n","*   train_df의 경우 리뷰 텍스트를 가지는 'document'칼럼에 Null이 일부 존재하므로 이 값은 공백으로 변환한다. \n","*   문자가 아닌 숫자의 경우 단어적인 의미로 부족하므로 파이썬의 정규 표현식 모듈인 re를 이용해 이 역시 공백으로 변환한다.\n","\n"],"metadata":{"id":"nn4HAloql8PN"}},{"cell_type":"code","source":["import re\n","\n","train_df = train_df.fillna(' ')\n","#정규 표현식을 이용해 숫자를 공백으로 변경(정규 표현식으로 \\d는 숫자를 의미함.)\n","train_df['document'] = train_df['document'].apply(lambda x :re.sub(r\"\\d+\",\" \",x))"],"metadata":{"id":"FrOky_aRl0EM","executionInfo":{"status":"ok","timestamp":1652079376885,"user_tz":-540,"elapsed":722,"user":{"displayName":"­신선민(자연과학대학 통계학과)","userId":"15538581907388309584"}}},"execution_count":63,"outputs":[]},{"cell_type":"code","source":["train_df.isnull().sum()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J8zslmbnms2f","executionInfo":{"status":"ok","timestamp":1652079384173,"user_tz":-540,"elapsed":313,"user":{"displayName":"­신선민(자연과학대학 통계학과)","userId":"15538581907388309584"}},"outputId":"e562d8a5-4c6c-496f-f599-1f17b703de08"},"execution_count":64,"outputs":[{"output_type":"execute_result","data":{"text/plain":["id          0\n","document    0\n","label       0\n","dtype: int64"]},"metadata":{},"execution_count":64}]},{"cell_type":"code","source":["#테스트 데이터 세트를 로딩하고 동일하게 Null 및 숫자를 공백으로 변환\n","test_df = pd.read_csv('/content/drive/MyDrive/ESAA(22-1)/Week10/네이버 영화 평점/ratings_test.txt',sep='\\t')\n","test_df = test_df.fillna(' ')\n","test_df['document'] = test_df['document'].apply(lambda x :re.sub(r\"\\d+\",\" \",x))\n","\n","#id 칼럼 삭제 수행\n","train_df.drop('id',axis=1,inplace=True)\n","test_df.drop('id',axis=1,inplace=True)"],"metadata":{"id":"bB6SzwYUmpkR","executionInfo":{"status":"ok","timestamp":1652079507901,"user_tz":-540,"elapsed":1234,"user":{"displayName":"­신선민(자연과학대학 통계학과)","userId":"15538581907388309584"}}},"execution_count":65,"outputs":[]},{"cell_type":"code","source":["test_df.isnull().sum()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nkfkWXtonLLp","executionInfo":{"status":"ok","timestamp":1652079518019,"user_tz":-540,"elapsed":4,"user":{"displayName":"­신선민(자연과학대학 통계학과)","userId":"15538581907388309584"}},"outputId":"8abbbebb-1d9d-4e12-eb16-ad0df4777184"},"execution_count":66,"outputs":[{"output_type":"execute_result","data":{"text/plain":["document    0\n","label       0\n","dtype: int64"]},"metadata":{},"execution_count":66}]},{"cell_type":"markdown","source":["\n","\n","1.   각 문장 한글 형태소 분석을 통해 형태소 단어로 토큰화(SNS 분석에 적합한 Twitter 클래스를 이용)\n","2.   TF-IDF 방식으로 단어 벡터화\n","\n"],"metadata":{"id":"8mWxvy0HnRop"}},{"cell_type":"code","source":["install.packages('k')"],"metadata":{"id":"GePrKOn5n0aQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install konlpy"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vi_OJzSDoEH7","executionInfo":{"status":"ok","timestamp":1652079748798,"user_tz":-540,"elapsed":7332,"user":{"displayName":"­신선민(자연과학대학 통계학과)","userId":"15538581907388309584"}},"outputId":"78bf169c-2e7c-4426-90fe-f14699630338"},"execution_count":68,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting konlpy\n","  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n","\u001b[K     |████████████████████████████████| 19.4 MB 1.3 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.21.6)\n","Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n","Collecting JPype1>=0.7.0\n","  Downloading JPype1-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (448 kB)\n","\u001b[K     |████████████████████████████████| 448 kB 27.7 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (4.2.0)\n","Installing collected packages: JPype1, konlpy\n","Successfully installed JPype1-1.3.0 konlpy-0.6.0\n"]}]},{"cell_type":"code","source":["from konlpy.tag import Twitter\n","\n","twitter = Twitter()\n","def tw_tokenizer(text):\n","    #입력 인자로 들어온 텍스트를 형태소 단어로 토큰화해 리스트 형태로 변환\n","    tokens_ko = twitter.morphs(text)\n","    return tokens_ko"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ri4-ip6AnN8b","executionInfo":{"status":"ok","timestamp":1652079752417,"user_tz":-540,"elapsed":2383,"user":{"displayName":"­신선민(자연과학대학 통계학과)","userId":"15538581907388309584"}},"outputId":"56f4e99a-a696-4714-a1ce-c35c5e456845"},"execution_count":69,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/konlpy/tag/_okt.py:17: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n","  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n"]}]},{"cell_type":"code","source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import GridSearchCV\n","\n","#Twitter 객체의 morphs() 객체를 이용한 tokenizer를 사용. ngram_range는 (1,2)\n","tfidf_vect = TfidfVectorizer(tokenizer=tw_tokenizer,ngram_range=(1,2),min_df=3,max_df=0.9)\n","tfidf_vect.fit(train_df['document'])\n","tfidf_matrix_train = tfidf_vect.transform(train_df['document'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SzUO7H8Mnyn-","executionInfo":{"status":"ok","timestamp":1652084326714,"user_tz":-540,"elapsed":1009387,"user":{"displayName":"­신선민(자연과학대학 통계학과)","userId":"15538581907388309584"}},"outputId":"bd7d7c3e-9805-4e92-a84a-32953b4ba292"},"execution_count":72,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n","  \"The parameter 'token_pattern' will not be used\"\n"]}]},{"cell_type":"code","source":["#로지스틱 회귀를 이용해 감성 분석 분류 수행\n","lg_clf = LogisticRegression(random_state=0)\n","\n","#파라미터 C 최적화를 위해 GridSearchCV를 이용\n","params = {'C': [1,3.5,4.5,5.5,10]}\n","grid_cv = GridSearchCV(lg_clf,param_grid=params,cv=3,scoring='accuracy',verbose=1)\n","grid_cv.fit(tfidf_matrix_train,train_df['label'])\n","print(grid_cv.best_params_,round(grid_cv.best_score_,4))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tnUl11DcouVo","executionInfo":{"status":"ok","timestamp":1652084607064,"user_tz":-540,"elapsed":128359,"user":{"displayName":"­신선민(자연과학대학 통계학과)","userId":"15538581907388309584"}},"outputId":"f957c892-5c3a-4bab-ac8f-1f518ae55483"},"execution_count":73,"outputs":[{"output_type":"stream","name":"stdout","text":["Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"]},{"output_type":"stream","name":"stdout","text":["{'C': 3.5} 0.8593\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"]}]},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score\n","\n","#학습 데이터를 적용한 TfidfVectorizer를 이용해 테스트 데이터를 TF-IDF 값으로 피처 변환함.\n","tfidf_matrix_test = tfidf_vect.transform(test_df['document'])\n","\n","#classifier는 GridSearchCV에서 최적 파라미터로 학습된 classifier를 그대로 이용\n","best_estimator = grid_cv.best_estimator_\n","preds = best_estimator.predict(tfidf_matrix_test)\n","\n","print('Logistic Regression 정확도:',accuracy_score(test_df['label'],preds))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lwaIGylYpaEh","executionInfo":{"status":"ok","timestamp":1652084851106,"user_tz":-540,"elapsed":180892,"user":{"displayName":"­신선민(자연과학대학 통계학과)","userId":"15538581907388309584"}},"outputId":"20e76cc2-d1f9-44b8-c08e-f98d89df6d8b"},"execution_count":75,"outputs":[{"output_type":"stream","name":"stdout","text":["Logistic Regression 정확도: 0.86186\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"5oZF-2xA6z3S"},"execution_count":null,"outputs":[]}]}